{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/Josh-Been/Sentiment-Per-Line/blob/master/Capture.PNG?raw=true \"Baylor University Libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI-Topic-Model\n",
    "\n",
    "Generates related keywords from a corpus bundeled together into topic areas. 10 keywords are generated per topic. A single text file is uploaded and each line is treated as a separate document.\n",
    "\n",
    "Baylor University Libraries: LSI Topic Model\n",
    "\n",
    "Implements the Latent Semantic Index\n",
    "\n",
    "From Wikipedia https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing \"Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.\"\n",
    "\n",
    "This Python application was built by the Baylor University Libraries to assist researchers to implement unsupervised topic modelling on 1-line documents, such as Twitter social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First**, ensure Anaconda 2.7 is installed on your system. If it is not, head to https://www.anaconda.com/download/ and install. Then continue with the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, launch the Jupyter Notebook application. If you do not see a launcher for Jupyter Notebook in the Anaconda application directory, launch Anaconda Navigator which will have a link to Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third**, download the Jupyter Notebook file https://raw.githubusercontent.com/Josh-Been/Twitter-Keyword-Search/master/Baylor-Libraries-Twitter-Keyword-Search.ipynb to your computer. In the Jupyter browser tab that opened in the previous step, click the Upload button and browse for the saved Jupyter Notebook file.\n",
    "\n",
    "Up to this point you have been reading an HTML version of this Notebook.\n",
    "\n",
    "Now switch to the interactive version in Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourth**, ensure you have the Gensim: Topic Modelling for Humans library installed. If you are confident you already installed Gensim, skip ahead of this step. Otherwise, put the cursor in the box below and click the 'run cell, select below' button at the top of this notebook.\n",
    "\n",
    "\n",
    "For background on Gensim - https://radimrehurek.com/gensim/\n",
    "\n",
    "#### NOTE: The command below may take a minute or two before any response is given, depending on the speed of the computer and the network connection. Please be patient before moving on to the next step. Now is a good time to go and grab that coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!conda install -c anaconda gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fifth**, browse for the text file containing lines of documents. Put the cursor in the box below and click the 'run cell, select below' button at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "from Tkinter import *\n",
    "from tkFileDialog import askopenfilename\n",
    "\n",
    "root = Tk()\n",
    "txt_file = askopenfilename()\n",
    "print txt_file\n",
    "root.update()\n",
    "root.destroy()\n",
    "\n",
    "documents = []\n",
    "documents[:] = []\n",
    "f = open(txt_file, 'r')\n",
    "for line in f:\n",
    "    documents.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sixth**, browse for a stop word list. This list must be a text file with one word per line. Put the cursor in the box beow and click the 'run cell, select below' button at the top of this notebook.\n",
    "\n",
    "There are numerous stopword lists on the internet. One example of lists in numerous languages is https://github.com/Alir3z4/stop-words It is advisable to modify lists as per your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "stoptxt = askopenfilename()\n",
    "print stoptxt\n",
    "root.update()\n",
    "root.destroy()\n",
    "\n",
    "stoplist = []\n",
    "stoplist[:] = []\n",
    "f1 = open(stoptxt, 'r')\n",
    "for line in f1:\n",
    "    stoplist.append(line.replace('\\n',''))\n",
    "f1.close()\n",
    "stoplist.append('rt')\n",
    "stoplist.append('&gt;')\n",
    "stoplist.append('sho')\n",
    "stoplist.append('&amp;:)')\n",
    "stopset = set(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seventh**, calculate the LSI topics for the corpus. Put the cursor in the box beow and click the 'run cell, select below' button at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if (word not in stopset and 'http' not in word and not word.isdigit() and word.islower())]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "# print texts\n",
    "# from pprint import pprint  # pretty-printer\n",
    "# pprint(texts)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('/tmp/twitter.dict')\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/twitter.mm', corpus)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "\n",
    "lsi.print_topics(3)\n",
    "\n",
    "# for doc in corpus_lsi:\n",
    "#     print doc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
